{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Names   Age  Total_Purchase  Account_Manager  Years  Num_Sites  \\\n",
      "0  Cameron Williams  42.0        11066.80                0   7.22        8.0   \n",
      "1     Kevin Mueller  41.0        11916.22                0   6.50       11.0   \n",
      "2       Eric Lozano  38.0        12884.75                0   6.67       12.0   \n",
      "3     Phillip White  42.0         8010.76                0   6.71       10.0   \n",
      "4    Cynthia Norton  37.0         9191.58                0   5.56        9.0   \n",
      "\n",
      "          Onboard_date                                           Location  \\\n",
      "0  2013-08-30 07:00:40      10265 Elizabeth Mission Barkerburgh, AK 89518   \n",
      "1  2013-08-13 00:38:46  6157 Frank Gardens Suite 019 Carloshaven, RI 1...   \n",
      "2  2016-06-29 06:20:07             1331 Keith Court Alyssahaven, DE 90114   \n",
      "3  2014-04-22 12:43:12       13120 Daniel Mount Angelabury, WY 30645-4695   \n",
      "4  2016-01-19 15:31:15                765 Tricia Row Karenshire, MH 71730   \n",
      "\n",
      "                       Company  Churn  \n",
      "0                   Harvey LLC      1  \n",
      "1                   Wilson PLC      1  \n",
      "2  Miller, Johnson and Wallace      1  \n",
      "3                    Smith Inc      1  \n",
      "4                   Love-Jones      1  \n"
     ]
    }
   ],
   "source": [
    "# Read the data from the CSV file and place it in a pandas DataFrame\n",
    "df = pd.read_csv(\"customer_churn.csv\")\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm data has been loaded correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Age  Total_Purchase     Years  Num_Sites  Account_Manager\n",
      "0    0.029936        0.417054  1.528446  -0.333235        -0.962910\n",
      "1   -0.133352        0.769905  0.963182   1.367585        -0.962910\n",
      "2   -0.623215        1.172234  1.096647   1.934526        -0.962910\n",
      "3    0.029936       -0.852432  1.128051   0.800645        -0.962910\n",
      "4   -0.786503       -0.361917  0.225198   0.233705        -0.962910\n",
      "..        ...             ...       ...        ...              ...\n",
      "895  0.029936        1.137369 -1.297874  -0.333235         1.038519\n",
      "896  1.662814       -0.070163  1.285069  -0.900175        -0.962910\n",
      "897  0.519800        0.828044  0.146690  -2.600995        -0.962910\n",
      "898  1.499527       -1.472556  0.154540   0.800645         1.038519\n",
      "899 -0.459927       -0.310398 -0.198750   0.800645         1.038519\n",
      "\n",
      "[900 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Columns to standardize\n",
    "col = ['Age', 'Total_Purchase', 'Years', 'Num_Sites', 'Account_Manager']\n",
    "\n",
    "# Standardize the selected columns\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df[col] = scaler.fit_transform(df[col])\n",
    "\n",
    "# Display the standardized columns\n",
    "print(df[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Data is clean*** \n",
      "\n",
      "Null values per column:\n",
      "Names              0\n",
      "Age                0\n",
      "Total_Purchase     0\n",
      "Account_Manager    0\n",
      "Years              0\n",
      "Num_Sites          0\n",
      "Onboard_date       0\n",
      "Location           0\n",
      "Company            0\n",
      "Churn              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in each column\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "if null_counts.sum() == 0:\n",
    "    print(\"***Data is clean*** \\n\")\n",
    "else:\n",
    "    print(\"***Data needs to be cleaned***\")\n",
    "\n",
    "# Display the number of null values for each column\n",
    "print(\"Null values per column:\")\n",
    "print(null_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode categorical columns using one-hot encoding\n",
    "# # Identify categorical columns\n",
    "# categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# # One-hot encode categorical columns\n",
    "# df_encoded = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# # Feature Extraction\n",
    "# # Extract date features from 'Onboard_date' column\n",
    "# if 'Onboard_date' in df_encoded.columns:\n",
    "#     df_encoded['Onboard_year'] = pd.to_datetime(df_encoded['Onboard_date']).dt.year\n",
    "#     df_encoded['Onboard_month'] = pd.to_datetime(df_encoded['Onboard_date']).dt.month\n",
    "#     df_encoded['Onboard_day'] = pd.to_datetime(df_encoded['Onboard_date']).dt.day\n",
    "#     df_encoded.drop(columns=['Onboard_date'], inplace=True)\n",
    "\n",
    "# # The DataFrame is now encoded and ready for feature extraction\n",
    "# print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (720, 9)\n",
      "X_test shape: (180, 9)\n",
      "y_train shape: (720,)\n",
      "y_test shape: (180,)\n"
     ]
    }
   ],
   "source": [
    "# Define the features and target variable\n",
    "X = df.drop(columns='Churn')  # Features (all columns except 'Churn')\n",
    "y = df['Churn']  # Target variable ('Churn' column)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate features and target\n",
    "# X = df_encoded.drop(columns='Churn')\n",
    "# y = df_encoded['Churn']\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train a basic Random Forest classifier\n",
    "# model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # Train the model on the training data\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model on the testing data\n",
    "# y_pred = model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# # Print the accuracy\n",
    "# print(f\"Model accuracy on testing data: {accuracy:.2f}\")\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model accuracy on testing data: 0.89\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94       148\n",
      "           1       0.76      0.59      0.67        32\n",
      "\n",
      "    accuracy                           0.89       180\n",
      "   macro avg       0.84      0.78      0.80       180\n",
      "weighted avg       0.89      0.89      0.89       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Separate features and target\n",
    "X = df_encoded.drop(columns='Churn')\n",
    "y = df_encoded['Churn']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "logistic_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Fit the model to the training data\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable for the testing data\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy of the model on the testing data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy and classification report\n",
    "print(f\"Logistic Regression model accuracy on testing data: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Purchase</th>\n",
       "      <th>Account_Manager</th>\n",
       "      <th>Years</th>\n",
       "      <th>Num_Sites</th>\n",
       "      <th>Onboard_date</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andrew Mccall</td>\n",
       "      <td>37.0</td>\n",
       "      <td>9935.53</td>\n",
       "      <td>1</td>\n",
       "      <td>7.71</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2011-08-29 18:37:54</td>\n",
       "      <td>38612 Johnny Stravenue Nataliebury, WI 15717-8316</td>\n",
       "      <td>King Ltd</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michele Wright</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7526.94</td>\n",
       "      <td>1</td>\n",
       "      <td>9.28</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2013-07-22 18:19:54</td>\n",
       "      <td>21083 Nicole Junction Suite 332, Youngport, ME...</td>\n",
       "      <td>Cannon-Benson</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jeremy Chang</td>\n",
       "      <td>65.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2006-12-11 07:48:13</td>\n",
       "      <td>085 Austin Views Lake Julialand, WY 63726-4298</td>\n",
       "      <td>Barron-Robertson</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Megan Ferguson</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6487.50</td>\n",
       "      <td>0</td>\n",
       "      <td>9.40</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2016-10-28 05:32:13</td>\n",
       "      <td>922 Wright Branch North Cynthialand, NC 64721</td>\n",
       "      <td>Sexton-Golden</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Taylor Young</td>\n",
       "      <td>32.0</td>\n",
       "      <td>13147.71</td>\n",
       "      <td>1</td>\n",
       "      <td>10.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2012-03-20 00:36:46</td>\n",
       "      <td>Unit 0789 Box 0734 DPO AP 39702</td>\n",
       "      <td>Wood LLC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jessica Drake</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8445.26</td>\n",
       "      <td>1</td>\n",
       "      <td>3.46</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2011-02-04 19:29:27</td>\n",
       "      <td>1148 Tina Stravenue Apt. 978 South Carlos TX 2...</td>\n",
       "      <td>Parks-Robbins</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Names   Age  Total_Purchase  Account_Manager  Years  Num_Sites  \\\n",
       "0   Andrew Mccall  37.0         9935.53                1   7.71        8.0   \n",
       "1  Michele Wright  23.0         7526.94                1   9.28       15.0   \n",
       "2    Jeremy Chang  65.0          100.00                1   1.00       15.0   \n",
       "3  Megan Ferguson  32.0         6487.50                0   9.40       14.0   \n",
       "4    Taylor Young  32.0        13147.71                1  10.00        8.0   \n",
       "5   Jessica Drake  22.0         8445.26                1   3.46       14.0   \n",
       "\n",
       "          Onboard_date                                           Location  \\\n",
       "0  2011-08-29 18:37:54  38612 Johnny Stravenue Nataliebury, WI 15717-8316   \n",
       "1  2013-07-22 18:19:54  21083 Nicole Junction Suite 332, Youngport, ME...   \n",
       "2  2006-12-11 07:48:13     085 Austin Views Lake Julialand, WY 63726-4298   \n",
       "3  2016-10-28 05:32:13      922 Wright Branch North Cynthialand, NC 64721   \n",
       "4  2012-03-20 00:36:46                    Unit 0789 Box 0734 DPO AP 39702   \n",
       "5  2011-02-04 19:29:27  1148 Tina Stravenue Apt. 978 South Carlos TX 2...   \n",
       "\n",
       "            Company  Churn  \n",
       "0          King Ltd    NaN  \n",
       "1     Cannon-Benson    NaN  \n",
       "2  Barron-Robertson    NaN  \n",
       "3     Sexton-Golden    NaN  \n",
       "4          Wood LLC    NaN  \n",
       "5     Parks-Robbins    NaN  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load new customers data\n",
    "pre = pd.read_csv('new_customers.csv')\n",
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Churn\nFeature names seen at fit time, yet now missing:\n- Company_Acosta PLC\n- Company_Adams, Dominguez and Vasquez\n- Company_Adams-Crawford\n- Company_Adams-Gibbs\n- Company_Aguilar, Kelly and Fuentes\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[194], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m new_data \u001b[38;5;241m=\u001b[39m pre\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNames\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompany\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnboard_date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Exclude non-feature columns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Make predictions on new data\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m s_pred \u001b[38;5;241m=\u001b[39m \u001b[43mlogistic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m s_pred\n",
      "File \u001b[1;32md:\\Visual Studio Workspace\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:451\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    450\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 451\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    453\u001b[0m     indices \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32md:\\Visual Studio Workspace\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:432\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    429\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    430\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 432\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[1;32md:\\Visual Studio Workspace\\.venv\\Lib\\site-packages\\sklearn\\base.py:580\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    511\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    517\u001b[0m ):\n\u001b[0;32m    518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \n\u001b[0;32m    520\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 580\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    585\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    586\u001b[0m         )\n",
      "File \u001b[1;32md:\\Visual Studio Workspace\\.venv\\Lib\\site-packages\\sklearn\\base.py:507\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    503\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n\u001b[1;32m--> 507\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Churn\nFeature names seen at fit time, yet now missing:\n- Company_Acosta PLC\n- Company_Adams, Dominguez and Vasquez\n- Company_Adams-Crawford\n- Company_Adams-Gibbs\n- Company_Aguilar, Kelly and Fuentes\n- ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pre[col] = scaler.transform(pre[col])\n",
    "\n",
    "# Ensure that the new data has the same columns as the training data\n",
    "new_data = pre.drop(['Names', 'Company', 'Onboard_date', 'Location'], axis=1)  # Exclude non-feature columns\n",
    "\n",
    "# Make predictions on new data\n",
    "s_pred = logistic_model.predict(new_data)\n",
    "s_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
